---
title: "Performance of Classification Methods in Pokemon Type Prediction"
author: |
  | Allen Zheng, Jerry Zhen, Justin Xu, Ryan Wu -- University of Waterloo
urlcolor: blue
link-citations: yes
citecolor: blue
linkcolor: blue
bibliography: references.bib
output: 
  bookdown::pdf_document2:
    toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
require()
```

# Summary

# Introduction

Pokémon is one of the most popular games worldwide. In it, players catch, train, and battle using monsters known as Pokemon. In particular, each Pokemon has one or two types. Types include elements, such as "Water" or "Fire", or more abstract characteristics such as "Fighting" or "Dragon". Each type has advantages and disadvantages against others in battle. As a result, Pokemon players share great interest in using effective types to win battles, so there is interest in predicting a Pokemon's type using other attributes. However, many of the previous attempts have failed as people have a limited number of observations due to game release periods or used limited attributes by choice. Thus, in order to make improvements on previous studies, we use classification techniques to re-examine the relationship between Pokemon types and their other attributes with an improved data set and a more diverse feature space. More importantly, we compare the performance between different models through nested CV and additionally attempt to predict Pokemon's secondary type if it exists.

# Dataset

## Data Scraping

Using [Pokédex Dataset](https://www.kaggle.com/datasets/mariotormo/complete-pokemon-dataset-updated-090420?select=pokedex_%28Update_04.21%29.csv) as an inspiration, we created our own scrapers with Python libraries `requests`, `beautifulsoup`, and `imageio` to scrape data from the Pokemon digital database [Pokémon DB](https://pokemondb.net/pokedex/all). The scraped attributes include `generation number`, `height`, `weight`, `combat` / `training` / `breeding` characteristics as well as the `weaknesses` (damage multipliers against other types) of each Pokemon. We also scraped images of each Pokemon and computed the imagery based data such as `size` and `RGB` value. Note that some features are eliminated as they posses challenges in model training or are irreverent in prediction. A detailed discussion on data processing can be found in Appendix.

## Feature introduction:

**Categorical features**:

| Column Name | Description                                                          |
|-------------|----------------------------------------------------------------------|
| status      | Whether the Pokémon is normal / legendary / sub-legendary / mythical |
| type_1      | The Primary Type of the Pokémon                                      |
| type_2      | The Secondary Type of the Pokémon if it exists                       |
| has_gender  | Whether the Pokémon has a gender                                     |

The types include: Bug, Dark, Dragon, Electric, Fairy, Fighting, Fire, Flying, Ghost, Grass, Ground, Ice, Normal, Poison, Psychic, Rock, Steel, Water \newpage

**Numerical features**:

| Feature Name          | Description                                                        |
|:----------------------|:-------------------------------------------------------------------|
| generation            | The chronological divisions of Pokémon, from 1st generation to 8th |
| type_number           | The number of types the Pokémon belong to, either 1 or 2           |
| height_m              | The height of the Pokémon in meters                                |
| weight_kg             | The weight of the Pokémon in kilograms                             |
| abilities_number      | The number of abilities possessed by the Pokémon                   |
| total_points          | Total number of base points                                        |
| hp                    | The base health points (hp) of the Pokémon                         |
| attack                | The base attack of the Pokémon                                     |
| defense               | The base defense of the Pokémon                                    |
| sp_attack             | The base special attack of the Pokémon                             |
| sp_defense            | The base special defense of the Pokémon                            |
| speed                 | The base speed of the Pokémon                                      |
| catch_rate            | Catch rate of the Pokémon                                          |
| base_friendship       | The base friendship of the Pokémon                                 |
| base_experience       | The base experience of a wild Pokémon when caught                  |
| maximum_experience    | The experience needed for the Pokémon to reach the maximum level   |
| egg_type_number       | The number of egg groups the Pokémon egg belongs to                |
| proportion_male       | The proportion of Pokémon that is male. Set to 50% if genderless.  |
| egg_cycles            | The number of cycles required to hatch an egg of the Pokémon       |
| damage_from\_**Type** | The damage multiplier when damaged by the move from a **Type**     |

**Imagery features (numerical)**:

| Feature Name         | Description                                                                     |
|:---------------------|:--------------------------------------------------------------------------------|
| size                 | The proportion of pixels occupied by the Pokémon's sprite                       |
| perimeter            | The number of pixels occupied by the sprite's boundary                          |
| perimeter_size_ratio | The ratio of the sprite's perimeter to its actual size                          |
| **Value**\_mean      | The mean of the **Value** pixel value over the entire sprite                    |
| **Value**\_sd        | The standard deviation of the **Value** pixel over the entire sprite            |
| vertical overflow    | The amount by which the sprite touches the boundaries of the image vertically   |
| horizontal overflow  | The amount by which the sprite touches the boundaries of the image horizontally |

**Value** = **Red**, **Green**, **Blue**, **Brightness**

# Previous Explorations

```{r child = 'previous-works.md'}
```

# Methods and Results

## Nested Cross-Validation

Acknowledging that the average `type_1` category has fewer than 60 Pokemon, we are working with a small amount of data. To make the most of this, we use stratified 5-fold nested cross validation so that every observation can take part in both the fitting and evaluation of models as well as introduce a more balanced as well as accurate representation of the type distribution. Using a fixed set of folds, we perform hyperparameter tuning on 4 folds at a time and use the final model to make predictions on the remaining fold. Through this process, we obtain one predicted type for each observation, and use this to produce an accuracy score and a confusion matrix.

## Hyperparameter Tuning

In each outer fold of nested cross validation, we perform another inner stratified 5-fold CV (cross validation) to select the best hyperparameters for the given model. However, for most models, the hyperparameter space is continuous and makes exhaustive search impossible. For example, regularized logistic regression admits a two-dimensional parameter space: $(0,{\infty})\times (0,{\infty})$.

In the early attempt, we tried grid search where we discretize the continuous search space and perform exhaustive search over the combination of hyperparameters in the search space. We then extracted the hyperparameters that optimizes the model in a stratified 5-fold CV and treat it as the optimal hyperparameters. For instance, we can reduce the search space of regularized logistic regression into $\{10^{-4}, 10^{-2}, 1, 10^2, 10^4\} \times \{10^{-4}, 10^{-2}, 1, 10^2, 10^4\}$ which contains $5^2 = 25$ points. However, there are two major drawbacks to this method. First, the hyperparameters in the discretized search space may be far from the true optimal parameters and second, the number of models needed to fit with a higher-dimensional hyperparameter spaces increases exponentially. This forces one to reduce the density of values in each dimension in order to keep the tuning process manageable.

To address the issue, we use Bayesian optimization (BayesOpt). Instead of a fixed set of values, we sample one point at a time from the parameter space, and evaluate the corresponding model's cross validation accuracy. We then treat the samples as data and fit a Gaussian process regression model to estimate the cross validation accuracy as a probabilistic function of the parameter space. In doing so, BayesOpt chooses the next sample according to a balance of exploration and exploitation. For exploration, BayesOpt targets points in regions with high uncertainty to gather information. For exploitation, BayesOpt targets points in regions with high expected accuracy to make incremental improvements. After some number of samples (50 in our case), BayesOpt reports the sample with the highest accuracy as the optimum. While it is not necessarily deterministic, it solves the problems faced by grid search. Namely, the optimum is not restricted to a predetermined set of points, and the algorithm adapts to any number of dimensions without forced exponential blowup. [@snoek2012practical]

## Logistic Regression

In Logistic Regression, we focused on multinomial logistic regression and used the Scikit-Learn package in Python. For multinomial logistic regression with $K \ge 2$ classes, we fit a single classifier with K outputs (one of them is 1), and take the softmax thereof. For regularization, ElasticNet encompasses both L1 and L2 penalties (and with a weak enough regularization term, no-penalty as well). We thus parameterize the search space of potential hyperparameters by the ratio of the L1 and L2 penalties, and the coefficient `C` representing the inverse of regularization strength. Specifically, we utilize Bayesian Optimization to sample over the entire continuous space with ranges of the ratio of the L1 and L2 penalties and `C` being $[0.0, 1.0]\times [10^{-4}, 10^{4}]$ .

After applying Bayesian Optimization and nested cross-validation with $k = 5$ folds, we observe that the ratio of the L1 and L2 penalties is constant at $1.0$ which indicates that lasso regularization or L1 penalty is chosen each time. The coefficient `C` fluctuates between the ranges of $[0, 5]$ indicating stronger regularization. Overall, we obtain a model accuracy of $96.302\%$.

We also examined the model performance without the features `type_2` and the `damage_from_{Type}` and used the same search space. We observe that the ratio of the L1 and L2 penalties fluctuate between the ranges of $[0.4, 1]$ and the coefficient `C` fluctuates between $[3.5, 10]$, indicating a stronger regularization. The model performance decreased significantly where the overall prediction accuracy decreased to $36.903\%$. As a result, it would suggest that the `type_2` and `damage_from_{TYPE}` variables are very significant features in predicting Pokemon type.

## SVM (Support Vector Machine)

We trained SVM models with the Scikit-Learn package in Python. The hyperparameters we tested are the regularization parameter `C`, the `kernel` type, the `degree` of the polynomial if the kernel is polynomial, the kernel coefficient `gamma`, an independent term in the kernel function `coef0` (only significant for a polynomial/sigmoid kernel), and whether to use probability output. For tuning, we consider the linear, polynomial, and radial basis function kernels. The sigmoid kernel is never guaranteed to be positive semi-definite and therefore, avoided. Moreover, the linear kernel is just a special case of the polynomial kernel (with `degree` = 1, `gamma` = 1, `coef0` = 0), allowing us to specify it together with the polynomial kernel. Thus the hyperparameter search space is presented as `degree`, `C`, `gamma`, and `coef0` ranging between $[2, 5]\times[10^{-4}, 10^{4}]\times[10^{-3}, 10^{3}]\times[0.0, 1.0]$ for the polynomial kernel. For the radial basis function kernel, the search space is `C` and `gamma` ranging between $[10^{-4}, 10^{4}] \times [10^{-3}, 10^{3}]$ respectively.

We decided that for the decision_function_shape, it makes the most sense to use one-versus-rest (OVR) rather than one-versus-one (OVO). This is because there are $18$ classes in the data set, and OVR will significantly reduce the complexity of SVM by only having 18 models, whereas OVO will create $18 \times 17 = 306$ models which would increase the model complexity, training time, and the risk of overfitting.

After applying Bayesian Optimization and nested cross-validation with $k = 5$ folds, we observe that in general, the best kernel was radial basis function with the regularization parameter `C` fluctuating from $33$ to $10,000$ where `C` is inversely proportional to the strength of regularization and `gamma` was between $[0.002, 0.036]$. Overall, we obtain a model accuracy of $96.868\%$.

Next, we investigate the strength of the model without the features `type_2` and the `damage_from_{TYPE}` without changing the search space. We observed that in general the best kernel was still the radial basis function, with `C` and `gamma` ranging from $[18, 867]\times[0.15, 0.27]$ respectively. After removing these features, the overall model accuracy falls to $46.206%$.

## Neural Network

We used PyTorch to build the Neural Network and tried to optimize the following hyperparameters: `H-layer`: number of hidden layers, `node`: number of nodes per layer, `drop`: dropout rate, `epoch`: number of epochs and `batch`: batch size. Note for simplicity and computation feasibility, we assumed a constant number of Nodes per layer. In addition, as Neural Network was built using PyTorch, we could not use Bayesian optimization but tried to optimize hyperparameters using a greedy algorithm approach and CV. Essentially, we do a grid search over the proposed search space for `number of hidden layers` `H-layer` and `node` (We fix a reasonable `drop`, `epoch`, `batch`) and another grid search over `drop`,`epcoh` and `batch` with "optimized" `H-layer` and `node`. Note we use CV to accurately evaluate the performance of the model with the hyperparameters from the proposed search space. We also mention that as we used randomly sampled batches to update weights and biases, the trained model contains randomness and may impact our judgment. Hence, we perform CV for multiple times ($5$ in our case) to average out the randomness. With the "optimized" hyperparameters, we conduct a nested CV to evaluate the algorithm performance where we randomly generate hyperparameters that is close to "optimized" hyperparameters in each inner CV in search of a potentially better model. (Note we will include the "optimized" hyperparameters in nested CV).

With all features included, we proposed the first search space `H-layer`$\times$`node` $= \{1,2,3,4,5\}\times\{100+25k_0\}$ for $k_0\in \{0,...,8\}$ (with `epoch` = 400, `batch` = 200, `drop` = 0.2) and second search space `epoch`$\times$`batch`$\times$`drop` $=\{300+50k_1\}\times\{100+25k_2\}\times\{0.2+0.025k_3\}$, $k_1\in \{0,...,5\}$, $k_2 \in \{0,...,6\}$ and $k_3 \in \{0,...,4\}$. The "optimized" hyper parameters is `H_layer` = 2, `node` = 250, `epoch` = 350, `batch` = 125, `drop` = 0.2. Note we found that and it appeared that optimization over `epoch`,`batch`,`drop` was necessary. The nested CV result shows an average of $98.957\%$ accuracy with two folds having $100\%$ accuracy. (Check appendix for optimization procedure)

We applied the same procedure removing `type_2` and `damage_from_{Type}`. The first search space is `H-layer`$\times$`node` $= \{1,2,3,4\}\times\{1000+100k_0\}$ for $k_0 \in \{0,...,10\}$ (with `epoch` = 600, `batch` = 200, `drop` = 0.2) and the second search space `epoch`$\times$`batch`$\times$`drop` $=\{100+50k_1\}\times\{100+25k_2\}\times\{0.2+0.025k_3\}$, $k_1 \in \{0,...,4\}$, $k_2 \in \{0,...,6\}$ and $k_3 \in \{0,...,4\}$. The "optimized" hyper parameters is `H_layer` = 2,`node` = 1300, `epoch` = 200, `batch` = 100, `drop` = 0.275. Note that it seems that smaller batch size increases the performance, this is likely caused by the noises in data that can only be captured with small batch. The nested CV result shows an average of $45.16\%$ accuracy with lowest fold having $40.758\%$ and highest fold having $49.763\%$ accuracy. This suggest the method might be unstable when applied to the dataset and could be affected by the randomness in fitting as well. (Check appendix for optimization procedure)

## Random Forest

For random forest, we utilized the Scikit-Learn package in python. The search space of hyperparameters we tested includes the number of random features to select at each split `max_features` as well as the maximum depth of the tree `max_depth`. The maximum features depend on the number of features in the dataset. Meanwhile, the maximum theoretical depth is as much as the number of observations, but in practice we do not need even close to that much. We also limit it to the number of features to make the optimization problem easier.

After applying Bayesian Optimization and nested cross-validation with k = 5 folds, we observe that in general the `gini` index criterion was preferred while the `max_depth` and `max_features` fluctuate between $[10, 38] \times [35, 74]$ respectively. Overall, we obtain a model accuracy of $96.491\%$.

Next we also investigate the strength of the model without the features `type_2` and the `damage_from_{Type}`, with the same search space as before. We observe that the `gini` index criterion was still preferred while the `max_depth` and `max_features` fluctuate between $[13, 37]\times[3, 37]$ respectively. Overall, we obtain a model accuracy of $44.971\%$ after removing these features.

# Results and Model Comparision

# Duplicated Data

# Discussion

# References {#sec:ref}

# Appendix
