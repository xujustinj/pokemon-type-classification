---
title: "Performance of Classification Methods in Pokemon Type Prediction"
author: |
  | Allen Zheng, Jerry Zhen, Justin Xu, Ryan Wu -- University of Waterloo
urlcolor: blue
link-citations: yes
citecolor: blue
linkcolor: blue
bibliography: references.bib
output:
  bookdown::pdf_document2:
    toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
library(grid)
library(gridExtra)
require(tinytex)
```

# Summary

Predicting a Pokemon's type is an inherently difficult task, with 18 possible categories and no clear pattern.
Therefore, there is great appeal in applying new methods in an attempt to achieve great accuracy.
In this report, we apply classification methods such as logistic regression, support vector machines (SVMs), random forests, and neural networks to an enhanced version of the Pokemon Database dataset.
For each model, we perform hyperparameter tuning using Bayesian Optimization and evaluate the tuning process using stratified nested cross-validation.
We find that the enhancements allow all models to achieve near-perfect accuracy.
Thus, we turn our attention to a subset of the features to better compare the models.
Further, to deal with sparse type categories, we investigate a "duplication" method for multi-label prediction, where a single-label classifier is trained on two copies of each Pokemon corresponding to each of the Pokemon's types.
**TODO: conclusion**

# Introduction

Pokémon is one of the most popular games worldwide.
In it, players catch, train, and battle using monsters known as Pokemon.
In particular, each Pokemon has one or two types such as the elemental type "Water" or the abstract type "Fighting".
Each type has advantages and disadvantages against others in battle.
As a result, Pokemon players share great interest in using effective types to win battles.
Many have attempted to predict a Pokemon's type using other attributes.
We would like to compare the methods they used in a standard setting, however their datasets contain different observations and features.
Here, we use an improved data set with a more diverse feature space.
Using a standard nested cross-validation method, we compare the performance between different models.

# Dataset

## Data Scraping

Using [Pokédex Dataset](https://www.kaggle.com/datasets/mariotormo/complete-pokemon-dataset-updated-090420?select=pokedex_%28Update_04.21%29.csv) as an inspiration, we created our own scrapers with Python libraries `requests`, `beautifulsoup`, and `imageio` to scrape data from the website [Pokémon DB](https://pokemondb.net/pokedex/all).
We chose numerical features, or categorical features with a small number of different values.
The scraped attributes include biometrics, combat statistics, and training characteristics as well as the `weaknesses` (damage multipliers against other types) of each Pokemon.
To augment this data, we scraped images of each Pokemon and computed simple characteristics of the image to serve as additional attributes.
A detailed discussion on data processing can be found in Appendix.

### Categorical and Binary Features

| Feature    | Description                                                          |
|:-----------|:---------------------------------------------------------------------|
| status     | Whether the Pokémon is normal, legendary, sub-legendary, or mythical |
| type_1     | The primary type of the Pokémon                                      |
| type_2     | The secondary type of the Pokémon if it exists                       |
| has_gender | Whether the Pokémon has a gender                                     |

The types include: Bug, Dark, Dragon, Electric, Fairy, Fighting, Fire, Flying, Ghost, Grass, Ground, Ice, Normal, Poison, Psychic, Rock, Steel, Water

\newpage

### Numerical Features

| Feature               | Description                                                        |
|:----------------------|:-------------------------------------------------------------------|
| generation            | The chronological divisions of Pokémon, from 1st generation to 8th |
| type_number           | The number of types the Pokémon belong to, either 1 or 2           |
| height_m              | The height of the Pokémon in meters                                |
| weight_kg             | The weight of the Pokémon in kilograms                             |
| abilities_number      | The number of abilities possessed by the Pokémon                   |
| total_points          | Total number of base points                                        |
| hp                    | The base health points (hp) of the Pokémon                         |
| attack                | The base attack of the Pokémon                                     |
| defense               | The base defense of the Pokémon                                    |
| sp_attack             | The base special attack of the Pokémon                             |
| sp_defense            | The base special defense of the Pokémon                            |
| speed                 | The base speed of the Pokémon                                      |
| catch_rate            | Catch rate of the Pokémon                                          |
| base_friendship       | The base friendship of the Pokémon                                 |
| base_experience       | The base experience of a wild Pokémon when caught                  |
| maximum_experience    | The experience needed for the Pokémon to reach the maximum level   |
| egg_type_number       | The number of egg groups the Pokémon egg belongs to                |
| proportion_male       | The proportion of Pokémon that is male, 50% if genderless          |
| egg_cycles            | The number of cycles required to hatch an egg of the Pokémon       |
| damage_from\_**Type** | The damage multiplier when damaged by the move from a **Type**     |

### Image Features (all numerical)

| Feature              | Description                                                                     |
|:---------------------|:--------------------------------------------------------------------------------|
| size                 | The proportion of pixels occupied by the Pokémon's sprite                       |
| perimeter            | The number of pixels occupied by the sprite's boundary                          |
| perimeter_size_ratio | The ratio of the sprite's perimeter to its actual size                          |
| **Value**\_mean      | The mean of the **Value** pixel value over the entire sprite                    |
| **Value**\_sd        | The standard deviation of the **Value** pixel over the entire sprite            |
| vertical overflow    | The amount by which the sprite touches the boundaries of the image vertically   |
| horizontal overflow  | The amount by which the sprite touches the boundaries of the image horizontally |

**Value** = **Red**, **Green**, **Blue**, **Brightness**

```{r, echo=FALSE, fig.align = 'left', out.width='100px', fig.width=10, fig.height=10}
knitr::include_graphics("pikachu.png")
```

# Previous Explorations

```{r child = 'previous-works.md'}
```

* [@pokemondecisiontree]: Simple analysis of the pokemon dataset from generations 1-8.
  Trying to predict `type_1` using pokemon stats, generation number, as well as mythical/legendary status.
* [@predictingpokemontypes]: Trying to predict pokemon `type_1` using naive bayes, random forest, and SVMs.
  Since flying type is severely underrepresented, they remove it and predict only 17 types.
  Uses pokemon stats, generation number, and legendary status.
  Additionally used grid search for hyper-parameter tuning, cross-validation, and leave one out cross-validation.
* [@whosthatnn]: Trying to predict dual pokemon types (`type_1` and `type_2`) using in game sprites from generation 1-5.
  Utilizes CNN for classification.
* [@predictingdualtypes]: Trying to predict dual pokemon types (`type_1` and `type_2`) using KNN.
  For pokemon with only one type, they imputed `type_2` as a repeat of `type_1`, however only predicted for one observation.
* [@multilabclasstf]: Trying to do multi-label classification with NN in TensorFlow using the in game sprite images.
* [@pokemontypepredstats]: Multi-label classification using MultiLabelBinarizer, basically one-hot encoding the 18 types and thus allowing it to predict more than one class.
  Another approach used is classifier chaining and label powersets.
  The data used only includes the stats of each Pokemon.
* [@whatsthatpokemon]: Multi-label classification of both types using CNN and fully-connected network (SNN) with pokemon in game sprite images.
* [@typeusingpokedexentries]: Multi-label classification of both types using natural language processing (NLP) on the pokemon’s pokedex descriptions.

# Methods and Results

## Nested Cross-Validation

Acknowledging that the average `type_1` category has fewer than 60 Pokemon, we are working with a small amount of data.
To make the most of this, we use 5-fold nested cross validation (CV) so that every observation can take part in both the fitting and evaluation of models.
The folds are stratified by `type_1` to ensure a similar type distribution in all folds.
For each train-test split, we perform hyperparameter tuning on the training set and use the tuned model to make predictions on the remaining fold.
Through this process, we obtain one predicted type for each Pokemon.
Comparing the predicted types to the actual types, we get an overall accuracy score for the model and plot a confusion matrix.

## Hyperparameter Tuning

For each train-test split of the outer CV, we perform another inner stratified 5-fold CV to select the best hyperparameters for the given model.
However, for most models, the hyperparameter space is continuous and makes exhaustive search impossible.
For example, regularized logistic regression admits a two-dimensional parameter space: $(0,{\infty})\times (0,{\infty})$.

In early attempts, we tried grid search where we discretize the continuous search space and perform exhaustive search over the combination of hyperparameters in the search space.
We then extracted the hyperparameters that optimize the model in a stratified 5-fold CV and treat it as the optimal hyperparameters.
For instance, we can reduce the search space of regularized logistic regression into $\{10^{-4}, 10^{-2}, 1, 10^2, 10^4\} \times \{10^{-4}, 10^{-2}, 1, 10^2, 10^4\}$ which contains $5^2 = 25$ points.
However, there are two major drawbacks to this method.
First, the hyperparameters in the discretized search space may be far from the true optimal parameters.
Second, the number of models needed to be fit increases exponentially with the number of dimensions in the search space.
This forces one to reduce the density of values in each dimension in order to keep the tuning process manageable.

To address the issue, we use Bayesian optimization (BayesOpt).[@snoek2012practical]
Instead of a fixed set of values, we sample one point at a time from the parameter space, and evaluate the corresponding model's cross validation accuracy.
We then treat the samples as data and fit a Gaussian process regression model to estimate the cross validation accuracy as a probabilistic function of the parameter space.
In doing so, BayesOpt chooses the next sample according to a balance of exploration and exploitation.
For exploration, BayesOpt targets points in regions with high uncertainty to gather information.
For exploitation, BayesOpt targets points in regions with high expected accuracy to make incremental improvements.
After some number of samples (50 in our case), BayesOpt reports the sample with the highest accuracy as the optimum.
While it is not necessarily deterministic, it solves the problems faced by grid search.
Namely, the optimum is not restricted to a predetermined set of points, and the algorithm adapts to any number of dimensions without forced exponential blowup.

## Logistic Regression

In Logistic Regression, we focused on multinomial logistic regression and used the Scikit-Learn package in Python.
For multinomial logistic regression with $K \ge 2$ classes, we fit a single classifier with K outputs (one of them is 1), and take the softmax thereof.
For regularization, ElasticNet encompasses both L1 and L2 penalties (and with a weak enough regularization term, no-penalty as well).
We thus parameterize the search space of potential hyperparameters by the ratio of the L1 and L2 penalties, and the coefficient `C` representing the inverse of regularization strength.
Specifically, we utilize Bayesian Optimization to sample over the entire continuous space with ranges of the ratio of the L1 and L2 penalties and `C` being $[0.0, 1.0]\times [10^{-4}, 10^{4}]$.

**TODO: selected hyperparameters**
After applying Bayesian Optimization and nested cross-validation with $k = 5$ folds, we observe that the ratio of the L1 and L2 penalties is constant at $1.0$ which indicates that lasso regularization or L1 penalty is chosen each time.
The coefficient `C` fluctuates between the ranges of $[0, 5]$ indicating stronger regularization.
Overall, we obtain a model accuracy of $96.302\%$.

We also examined the model performance without the features `type_2` and the `damage_from_{Type}` and used the same search space.
We observe that the ratio of the L1 and L2 penalties fluctuate between the ranges of $[0.4, 1]$ and the coefficient `C` fluctuates between $[3.5, 10]$, indicating a stronger regularization.
The model performance decreased significantly where the overall prediction accuracy decreased to $36.903\%$.
As a result, it would suggest that the `type_2` and `damage_from_{Type}` variables are very significant features in predicting Pokemon type.

## SVM (Support Vector Machine)

We trained SVM models with the Scikit-Learn package in Python.
The hyperparameters we tested are the regularization parameter `C`, the `kernel` type, the `degree` of the polynomial if the kernel is polynomial, the kernel coefficient `gamma`, an independent term in the kernel function `coef0` (only significant for a polynomial/sigmoid kernel), and whether to use probability output.
For tuning, we consider the linear, polynomial, and radial basis function kernels.
The sigmoid kernel is never guaranteed to be positive semi-definite and therefore, avoided.
Moreover, the linear kernel is just a special case of the polynomial kernel (with `degree` = 1, `gamma` = 1, `coef0` = 0), allowing us to specify it together with the polynomial kernel.
Thus the hyperparameter search space is presented as `degree`, `C`, `gamma`, and `coef0` ranging between $[2, 5]\times[10^{-4}, 10^{4}]\times[10^{-3}, 10^{3}]\times[0.0, 1.0]$ for the polynomial kernel.
For the radial basis function kernel, the search space is `C` and `gamma` ranging between $[10^{-4}, 10^{4}] \times [10^{-3}, 10^{3}]$ respectively.

We decided that for the decision_function_shape, it makes the most sense to use one-versus-rest (OVR) rather than one-versus-one (OVO).
This is because there are $18$ classes in the data set, and OVR will significantly reduce the complexity of SVM by only having 18 models, whereas OVO will create $18 \times 17 = 306$ models which would increase the model complexity, training time, and the risk of overfitting.

After applying Bayesian Optimization and nested cross-validation with $k = 5$ folds, we observe that in general, the best kernel was radial basis function with the regularization parameter `C` fluctuating from $33$ to $10,000$ where `C` is inversely proportional to the strength of regularization and `gamma` was between $[0.002, 0.036]$.
Overall, we obtain a model accuracy of $96.868\%$.

Next, we investigate the strength of the model without the features `type_2` and the `damage_from_{Type}` without changing the search space.
We observed that in general the best kernel was still the radial basis function, with `C` and `gamma` ranging from $[18, 867]\times[0.15, 0.27]$ respectively.
After removing these features, the overall model accuracy falls to $46.206%$.

## Neural Network

We used PyTorch to build the Neural Network and tried to optimize the following hyperparameters: `H-layer`: number of hidden layers, `node`: number of nodes per layer, `drop`: dropout rate, `epoch`: number of epochs.
Note for simplicity and computation feasibility, we assumed a constant number of Nodes per layer.
In addition, as Neural Network was built using PyTorch, we could not use Bayesian optimization but tried to optimize hyperparameters using a greedy algorithm approach and CV.
Essentially, we do a grid search over the proposed search space for `number of hidden layers` `H-layer` and `node` (We fix a reasonable `drop`, `epoch`, `batch`) and another grid search over `drop`,`epcoh` and `batch` with "optimized" `H-layer` and `node`.
Note we use CV to accurately evaluate the performance of the model with the hyperparameters from the proposed search space.
We also mention that as we used randomly sampled batches to update weights and biases, the trained model contains randomness and may impact our judgment.
Hence, we perform CV for multiple times ($5$ in our case) to average out the randomness.
With the "optimized" hyperparameters, we conduct a nested CV to evaluate the algorithm performance where we randomly generate hyperparameters that is close to "optimized" hyperparameters in each inner CV in search of a potentially better model.
(Note we will include the "optimized" hyperparameters in nested CV).

With all features included, we proposed the first search space `H-layer`$\times$`node` $= \{1,2,3,4,5\}\times\{100+25k_0\}$ for $k_0\in \{0,...,8\}$ (with `epoch` = 400, `batch` = 200, `drop` = 0.2) and second search space `epoch`$\times$`batch`$\times$`drop` $=\{300+50k_1\}\times\{100+25k_2\}\times\{0.2+0.025k_3\}$, $k_1\in \{0,...,5\}$, $k_2 \in \{0,...,6\}$ and $k_3 \in \{0,...,4\}$.
The "optimized" hyper parameters is `H_layer` = 2, `node` = 250, `epoch` = 350, `batch` = 125, `drop` = 0.2.
Note we found that and it appeared that optimization over `epoch`,`batch`,`drop` was necessary.
The nested CV result shows an average of $98.957\%$ accuracy with two folds having $100\%$ accuracy.
(Check appendix for optimization procedure)

We applied the same procedure removing `type_2` and `damage_from_{Type}`.
The first search space is `H-layer`$\times$`node` $= \{1,2,3,4\}\times\{1000+100k_0\}$ for $k_0 \in \{0,...,10\}$ (with `epoch` = 600, `batch` = 200, `drop` = 0.2) and the second search space `epoch`$\times$`batch`$\times$`drop` $=\{100+50k_1\}\times\{100+25k_2\}\times\{0.2+0.025k_3\}$, $k_1 \in \{0,...,4\}$, $k_2 \in \{0,...,6\}$ and $k_3 \in \{0,...,4\}$.
The "optimized" hyper parameters are `H_layer` = 2,`node` = 1300, `epoch` = 200, `drop` = 0.275.
The nested CV result shows an average of $45.16\%$ accuracy with lowest fold having $40.758\%$ and highest fold having $49.763\%$ accuracy.
This suggest the method might be unstable when applied to the dataset and could be affected by the randomness in fitting as well.
(Check appendix for optimization procedure)

## Random Forest

For random forests, we utilized the Scikit-Learn package in Python.
The search space of hyperparameters we tested include

- `max_features`: the number of random features to try at each split.
- `max_depth`: the maximum depth of the tree.

The maximum features depend on the number of features in the dataset.
Meanwhile, the maximum theoretical depth is as much as the number of observations, but in practice we do not need even close to that much.
We also generously limit it to the number of features, to make the optimization problem easier.

After hyperparameter tuning, we observe that in general the `gini` index criterion was preferred.
while the `max_depth` and `max_features` fluctuate between $[10, 38] \times [35, 74]$ respectively.
Overall, we obtain a model accuracy of $96.491\%$.

Next we also investigate the strength of the model without the features `type_2` and the `damage_from_{Type}`, with the same search space as before.
We observe that the `gini` index criterion was still preferred while the `max_depth` and `max_features` fluctuate between $[13, 37]\times[3, 37]$ respectively.
Overall, we obtain a model accuracy of $44.971\%$ after removing these features.
